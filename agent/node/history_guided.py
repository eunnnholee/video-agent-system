from pathlib import Path
import logging
from jinja2 import Template
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from modules.recommender import find_most_similar_prompt
from modules.question_generator import suggest_followup_questions

logger = logging.getLogger(__name__)

# ì „ì—­ ì„¤ì •
TEMPLATE_PATH = Path("agent/templates/history_guided_prompt.jinja")
LLM = ChatOpenAI(model="gpt-4", temperature=0.7, max_tokens=800)

def load_template(path: Path) -> Template:
    return Template(path.read_text(encoding="utf-8"))

JINJA_TEMPLATE = load_template(TEMPLATE_PATH)

def handle_empty_input() -> dict:
    return {"history_guided_prompt": "â— ì‚¬ìš©ìž ìž…ë ¥ì´ ë¹„ì–´ ìžˆìŠµë‹ˆë‹¤."}

def handle_no_similar_prompt() -> dict:
    logger.warning("ìœ ì‚¬í•œ ê³¼ê±° í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    fallback_intents = ["ë‹¨ìˆœí•œ í‘œí˜„ ìˆ˜ì • ë˜ëŠ” ë¬¸ë²• ë³€ê²½"]
    questions = suggest_followup_questions(fallback_intents)
    return {
        "history_guided_prompt": "\n".join([
            "âŒ ìœ ì‚¬í•œ ê³¼ê±° í”„ë¡¬í”„íŠ¸ê°€ ì—†ì–´ ìµœì í™”ëœ ì¶”ì²œì´ ì–´ë µìŠµë‹ˆë‹¤.",
            "ðŸ’¡ ì•„ëž˜ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ íŽ¸ì§‘í•´ë³´ëŠ” ê±´ ì–´ë– ì„¸ìš”?",
            *[f"- {q}" for q in questions]
        ])
    }

def render_prompt(original: str, edited: str, diff_json: dict, intentions: list, user_input: str) -> str:
    return JINJA_TEMPLATE.render(
        original=original,
        edited=edited,
        diff_json=diff_json,
        intentions=intentions,
        user_input=user_input
    )

def generate_history_guided_prompt(state: dict) -> dict:
    user_input = state.get("user_input", "").strip()
    if not user_input:
        return handle_empty_input()

    result = find_most_similar_prompt(user_input)
    if result is None:
        return handle_no_similar_prompt()

    original, edited, diff_json, intentions, _ = result

    rendered_prompt = render_prompt(original, edited, diff_json, intentions, user_input)
    logger.debug(f"ðŸ§¾ Rendered prompt for LLM:\n{rendered_prompt}")

    response = LLM.invoke([{"role": "user", "content": rendered_prompt}])
    return {"history_guided_prompt": response.content}

